{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Restaurant Visitor Forecasting\n\n\nRestaurants need to know the number of visitors each day to plan for supplies and personnel. This is however a difficult task due to many unpredictable factors. Recruit Holdings has brought data related to Restaurants, Restaurant Reservations & Restaurant Visits from the following sites, which will be our input data for analysis:\n\n*    Hot Pepper Gourmet: A Restaurant-rating service similar to Yelp!\n*    AirREGI:  Reservation control and cash register used at Restaurants to make transactions\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n## 1. Goal of Competition\nPredict total number of visitors to a restaurant for specified future dates\n\n## 2. Dataset Summary\n\n*   Data is orignated from 2 systems - prefaced with air or hpg\n*   Restaurant has unique Id - either a air_store_id or hpg_store_id\n*   Not all restaurants are covered by both systems\n*   Restaurant data more than what is required for prediction is provided\n*   Latitude & Longitude are not exact\n*   Training Dataset: Data from January 2016 to early (first week) April 2017\n*   Test Dataset: Predict Visitors for mid weeks (second and third weeks) of April 2017\n*   The training and testing set both omit days where the restaurants were closed.       \n"},{"metadata":{},"cell_type":"markdown","source":"## 3. Setup Environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport seaborn as sns\n\nfrom datetime import datetime\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"hpg_store_info = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/hpg_store_info.csv\")\nhpg_reserve = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/hpg_reserve.csv\")\nair_store_info = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/air_store_info.csv\")\nair_reserve = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/air_reserve.csv\")\nstore_id_relation = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/store_id_relation.csv\")\ndate_info = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/MetaData/MetaData/date_info.csv\")\n\ntest = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/sample_submission.csv\")\ntest_ = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/test.csv\")\ntrain = pd.read_csv(\"/kaggle/input/restaurant-visitor-forecasting/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Data Files & Description\n\n*    air_store_info.csv: Information about stores in air system\n*    hpg_store_info.csv: Information about stores in hpg system\n*    store_id_relation.csv: Mapping between stores in air and hpg system\n*    air_reserve.csv: Reservations made in air system\n*    hpg_reserve.csv: Reservations made in hpg system\n*    date_info.csv: Holidays in Japan for the period relevant to dataset\n*    train.csv: Historical visit data for air restaurants\n*    test.csv: Future restaurant visit date for which number of visitors are required to be predicted "},{"metadata":{},"cell_type":"markdown","source":"### 5.1. Basic Overview:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total restaurants common in AIR and HPG: \",len(store_id_relation))\n\nprint(\"\\n\\nAIR dataset overview\")\nprint(\"Total number of entries in air_reserve dataset: \", air_reserve.shape)\nprint(\"Total number of restaurants in air_reserve dataset: \", len(air_reserve.air_store_id.unique()))\nprint(\"Total unique genre in AIR restaurants: \",len(air_store_info.air_genre_name.unique()))\nprint(\"Total number of AIR restaurant's locations: \",len(air_store_info.air_area_name.unique()))\n\nprint(\"\\n\\nHPG dataset overview\")\nprint(\"Total number of entries in hpg_reserve dataset: \", hpg_reserve.shape)\nprint(\"Total number of restaurants in hpg_reserve dataset: \", len(hpg_reserve.hpg_store_id.unique()))\nprint(\"Total unique genre in HPG restaurants: \",len(hpg_store_info.hpg_genre_name.unique()))\nprint(\"Total number of HPG restaurant's locations: \",len(hpg_store_info.hpg_area_name.unique()))\n\nprint(\"\\n\\nTraining dataset overview\")\nprint(\"Total number of entries in train dataset: \", train.shape)\nprint(\"Total number of unique AIR restaurants in the train set: \",len(train.air_store_id.unique()))\nprint(\"Average daily visitors: \",train.visitors.mean())\nprint(\"Training data duration:- \", train.visit_date.min(), \" to \" ,train.visit_date.max())\n\nprint(\"\\n\\nTest dataset overview\")\nprint(\"Total number of entries in test dataset: \", test_.shape)\nprint(\"Total unique restaurants:-\",len(test_.air_store_id.unique()))\nprint(\"Test data duration: \", test_.visit_date.min(), \" to \",test_.visit_date.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. Null Value Analysis : "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AIR_STORE_INFO:\")\nprint(air_store_info.isnull().sum())\n\nprint(\"\\nHPG_STORE_INFO:\")\nprint(hpg_store_info.isnull().sum())\n\nprint(\"\\nAIR_RESERVE:\")\nprint(air_reserve.isnull().sum())\n\nprint(\"\\nHPG_RESERVE:\")\nprint(hpg_reserve.isnull().sum())\n\nprint(\"\\nSTORE_ID_RELATION:\")\nprint(store_id_relation.isnull().sum())\n\n\nprint(\"\\nDATE_INFO:\")\nprint(date_info.isnull().sum())\n\n\nprint(\"\\nTRAIN:\")\nprint(train.isnull().sum())\n\nprint(\"\\nTEST:\")\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n\n*    From the above outputs we can observe that there are **no null values** in the dataset.\n*    There is no need of performing any kind of missing data imputation.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.3. Preprocessing the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing air visitors from TRAIN dataset\n\nair_data = pd.merge(train,air_store_info,how='left', on=['air_store_id']) # merging dataframes\ndate_info.rename(columns={'calendar_date':'visit_date'},inplace=True)  # renaming columns\nair_data = pd.merge(air_data,date_info,how='left', on=['visit_date'])\nair_data.sort_values(by='visit_date',ignore_index=True,inplace=True)\n\nair_data['visit_date'] = pd.to_datetime(air_data['visit_date'])\n# air_data['day'] = air_data['visit_date'].dt.day\n# air_data['dow'] = air_data['visit_date'].dt.weekday\n# air_data['year'] = air_data['visit_date'].dt.year\n# air_data['month'] = air_data['visit_date'].dt.month\nair_data['month_name'] = air_data['visit_date'].dt.month_name()\n# air_data['week'] = air_data['visit_date'].dt.isocalendar().week\n# air_data['quarter'] = air_data['visit_date'].dt.quarter\nair_data['visit_date'] = air_data['visit_date'].dt.date\n# air_data.to_csv('air_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*   We are merging the related datasets with the training data  in  the **air_data** dataframe to get all the features for further comparisions with the number of visitors.\n*   All the date-related informations are extracted from the visit_date column and added to air_data dataframe as new features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing AIR reservation data\nair_reserve_data = pd.merge(air_reserve,air_store_info,how='left', on=['air_store_id'])\n\nair_reserve_data['visit_datetime'] = pd.to_datetime(air_reserve_data['visit_datetime'])\nair_reserve_data['visit_hour'] = air_reserve_data['visit_datetime'].dt.hour\nair_reserve_data['visit_date'] = air_reserve_data['visit_datetime'].dt.date\n\nair_reserve_data['reserve_datetime'] = pd.to_datetime(air_reserve_data['reserve_datetime'])\nair_reserve_data['reserve_hour'] = air_reserve_data['reserve_datetime'].dt.hour\nair_reserve_data['reserve_date'] = air_reserve_data['reserve_datetime'].dt.date\n\n#calculate reservation time difference \nair_reserve_data['res_vis_diff'] = (air_reserve_data['visit_datetime']-air_reserve_data['reserve_datetime']).apply(\n    lambda x : x.total_seconds()/3600)\n\nair_reserve_data.rename(columns={'reserve_visitors':'air_reserve_visitors'},inplace=True)\nair_reserve_data.to_csv('air_reserve_data.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_reserve_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*   The air_reserve data is merged with air_store_info to get the area ,genre, latitude and longitude for the corresponding air_store_id into **air_reserve_data** dataframe.\n*   The visit_datetime column has been changed into datetime from object dtype."},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing HPG reservation data\nhpg_reserve_data = pd.merge(hpg_reserve,store_id_relation,on=['hpg_store_id'],how='inner')\nhpg_reserve_data = pd.merge(hpg_reserve_data,hpg_store_info,on=['hpg_store_id'],how='left')\n\nhpg_reserve_data['visit_datetime'] = pd.to_datetime(hpg_reserve_data['visit_datetime'])\nhpg_reserve_data['visit_hour'] = hpg_reserve_data['visit_datetime'].dt.hour\nhpg_reserve_data['visit_date'] = hpg_reserve_data['visit_datetime'].dt.date\n\nhpg_reserve_data['reserve_datetime'] = pd.to_datetime(hpg_reserve_data['reserve_datetime'])\nhpg_reserve_data['reserve_hour'] = hpg_reserve_data['reserve_datetime'].dt.hour\nhpg_reserve_data['reserve_date'] = hpg_reserve_data['reserve_datetime'].dt.date\n\n#calculate reservation time difference \nhpg_reserve_data['res_vis_diff'] = (hpg_reserve_data['visit_datetime']-hpg_reserve_data['reserve_datetime']).apply(\n    lambda x : x.total_seconds()/3600)\n\nhpg_reserve_data.rename(columns={'reserve_visitors':'hpg_reserve_visitors'},inplace=True)\nhpg_reserve_data.to_csv('hpg_reserve_data.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpg_reserve_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*   The hpg_reserve data is merged with store_relation_info to get the corresponding air_store_id into **hpg_reserve_data** dataframe.\n*   It is then merged with the hpg_store_info to get the area, genre, latitude and longitude for each hpg_store_id.   \n*   The visit_datetime column has been changed into datetime from object dtype."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_air_reserve = air_reserve_data.groupby(['air_store_id','visit_date'],as_index=False)['air_reserve_visitors'].sum()\ntotal_hpg_reserve =  hpg_reserve_data.groupby(['air_store_id','visit_date'],as_index=False)['hpg_reserve_visitors'].sum()\nair_data = pd.merge(air_data,total_air_reserve,on=['air_store_id','visit_date'],how='left')\nair_data = pd.merge(air_data,total_hpg_reserve,on=['air_store_id','visit_date'],how='left')\nair_data.fillna(value=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  The number of reserve visitors for each restaurant per day is kept in the total_air_reserve and total_hpg_reserve dataframes.\n*  The air and hpg reserve visitor information is taken from their individual reserve dataset and merged with the air_data for comparing with the number of visitors. "},{"metadata":{},"cell_type":"markdown","source":"### 6. Individual Data Visualization\n\nNow, we will try to analyze the different features for every individual datasets to get a better understanding of the data provided to us."},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Train data\n\n#### 6.1.1. Number of visitors each day in the training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt1 = air_data.groupby(['visit_date'], as_index=False).agg({'visitors': np.sum})\nplt1=plt1.set_index('visit_date')\nplt1.plot(figsize=(15, 6))\nplt.ylabel(\"All Visitors\")\nplt.title(\"Visitor each day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*         The above plot depits the fact that there is almost 150% hike in the number of restaurants during mid of 2016.\n*         The reason behind the hike might be caused by including new restaurants to the AIR database in mid 2016.\n*         The ups and downs that we see might be due to the weekday and weekends.\n*         The sharp decrese on 1st of Jan is due to the new-year's eve, as most of the restaurants stay closed on new-years day."},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.2. Average number of visitors per restaurant per day"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt2 = air_data.groupby(['air_store_id'])['visitors'].mean().to_frame()\nf,ax = plt.subplots(1,1, figsize=(10,6))\nplt.hist(x=plt2.visitors.values, bins =100)\nplt.xlabel('Visitors')\nplt.ylabel('Count')\nplt.title('Average visitors per restaurant per day')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n\n*        The number of average visitors per restaurant per day almost follows a normal curve with mean visitors approx. 20, having a slight right skewness.\n*        There are a large number of restaurants which have capacity less than 20. It explains the fact that there is a large number of small restaurants in Japan.\n*        There are very few luxurious restaurants that are visited by large number of people (>120).\n\nThe following boxplot gives a better analysis of the same data to understand the ranges and outliers in air_data."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = air_data.groupby(['air_store_id'])['visitors'].mean().to_frame()\nf,ax = plt.subplots(1,1, figsize=(10,6))\nsns.boxplot(y='visitors', data=temp,ax=ax)\nplt.title('Boxplot of average visitors per restaurant per day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*   The minimum number of visitors that we can observe from this plot is almost reaching zero.\n*   The mean of the visitors is 20(approx).\n*   The maximum number of visitors is between 55-60.\n*   We observe certain very high values (outlier) greater than 60 and and even greater than 100 visitors.\n*   25th percentile and 75th percentile values are 13(approx) and 30(approx) respectively.\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.3. PDF of average reserve visitors per restaurant"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_1 = total_air_reserve.groupby(['air_store_id'],as_index=False)['air_reserve_visitors'].mean()\ntemp_2 = total_hpg_reserve.groupby(['air_store_id'],as_index=False)['hpg_reserve_visitors'].mean()\n\nf,ax = plt.subplots(1,1, figsize=(10,6))\nsns.distplot(a=temp_1.air_reserve_visitors.values, ax=ax, label='air_reserve_visitors')\nsns.distplot(a=temp_2.hpg_reserve_visitors.values, ax=ax, label='hpg_reserve_visitors')\nplt.xlabel('Visitors Reserved')\nplt.ylabel('Density')\nplt.legend()\nplt.title('PDF of average reserve visitors per restaurant')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n\n*    The spread of AIR reservations is higher than that of HPG reservations.\n*    There is a large number of reservations in HPG with visitors count between 5 to 10.\n*    There are few reservations in HPG where the visitors count is more than 20 or even reaching 40.\n*    Even in AIR, the maximum number of visitors registered is 40, but the number of registrations are more than that of HPG.\n*    The number of unregistered visitors is far more than the number of registered visitors.\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.2. Air Reservation Data\n\n#### 6.2.1 Number of reserve visitors per day"},{"metadata":{"trusted":true},"cell_type":"code","source":"airR1 = air_reserve_data.groupby(['visit_date'], as_index=False).agg({'air_reserve_visitors': np.sum})\nairR1=airR1.set_index('visit_date')\nairR1.plot(figsize=(15, 6))\nplt.ylabel(\"Number of Reserve Visitors\")\nplt.title(\"Reserve Visitors each day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*   There were much fewer reservations made in 2016 through the air system. \n*   The volume only increased during the end of that year during the month of festivities. \n*   In 2017 the visitor numbers stayed strong. \n*   We can see a flat line after July 2016 in the number of reserve visitors which shows that there is no data between August to October, 2016.\n*   There is a drastic fall in number of reserve visitors on 1st January as most of the restaurants stay closed on new-years day.\n"},{"metadata":{},"cell_type":"markdown","source":"#### 6.2.2. Visit Hour and Reservation-Visit Hour Gap analysis for air_reserve"},{"metadata":{"trusted":true},"cell_type":"code","source":"airR2 = air_reserve_data.groupby(['visit_hour'], as_index=False).agg({'air_reserve_visitors': np.sum})\nairR3 = air_reserve_data.groupby(['res_vis_diff'], as_index=False).agg({'air_reserve_visitors': np.sum})\n\n#plot\nfig = plt.figure(figsize=(20, 6)) \ngs = gridspec.GridSpec(1, 2, width_ratios=[1.5, 2.5]) \nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nsns.barplot(x=\"visit_hour\",y=\"air_reserve_visitors\",data=airR2,ax=ax0)\nsns.barplot(x=\"res_vis_diff\",y=\"air_reserve_visitors\",data=airR3[(airR3['res_vis_diff'] <= 80)],ax=ax1)\nax0.set_xlabel(\"Visit Hour\")\nax0.set_ylabel(\"Number of Reserve Visitor\")\nax1.set_xlabel('Reservation-Visit Hour Gap')\nax1.set_ylabel(\"Number of Reserve Visitor\")\n\nfor ax in [ax0,ax1]:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90) \n        \nplt.tight_layout(pad = 3.0)        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations on Visit Hour:-**\n*   Most people make reservation for dinner (6pm to 8pm).\n*   There are very few reservations in the morning.\n\n**Observations on Reservation-Visit Hour Gap:-**\n*   Making reservation just a few hours before visiting the restaurant is the most common practice.\n*   The plot shows that there is a nice 24-hour gap pattern between reservation and visit.\n*   Very long time gaps between reservation and visit are not uncommon."},{"metadata":{},"cell_type":"markdown","source":"#### 6.2.3. Number of restaurants per area covered by AIR_RESERVE dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of restaurent in area: Air Data\nairS1 = air_reserve_data['air_area_name'].value_counts().reset_index().sort_index()\nairS2 = air_reserve_data['air_genre_name'].value_counts().reset_index().sort_index()\nfig,ax = plt.subplots(1,2)\nsns.barplot(y='index' ,x='air_area_name',data=airS1.iloc[:15],ax=ax[0])\nsns.barplot(y='index' ,x='air_genre_name',data=airS2.iloc[:15],ax=ax[1])\nfig.set_size_inches(20,7, forward=True)\nax[0].set_ylabel('Number of Restaurent')\nax[1].set_ylabel('Number of Restaurent')\nplt.tight_layout(pad=3.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations on area:-**\n*    The restaurants in Japan are spreaded over 71 areas out of which the top 15 areas are shown above.\n*    Tōkyō-to Shibuya-ku Shibuya has the largest number of restaurants.\n\n**Observations on genre:-**\n*    The restaurants in Japan is subdivided in 14 food genres.\n*    Findings from the air_reserve dataset shows that Izakaya is the most popular genre in Japan. The second most popular genre in Japan is Italian/French.\n*    International cuisine,Asian and Karaoke/Party are the least preferred genre.\n\nTo start a restaurant business in Japan, choosing restaurant area and food genre will be an important decision."},{"metadata":{},"cell_type":"markdown","source":"### 6.3. HPG Reservation Data\n#### 6.3.1. Number of reserve visitors per data"},{"metadata":{"trusted":true},"cell_type":"code","source":"hpgR1 = hpg_reserve_data.groupby(['visit_date'], as_index=False).agg({'hpg_reserve_visitors': np.sum})\nhpgR1=hpgR1.set_index('visit_date')\nhpgR1.plot(figsize=(15, 6))\nplt.ylabel(\"Sum of Visitors\")\nplt.title(\"Visitor each day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*   The hpg reservation follow a more orderly pattern.\n*   There is a clear spike in Dec 2016 which might caused due to festivities at that time of year."},{"metadata":{},"cell_type":"markdown","source":"#### 6.3.2. Visit Hour and Reservation-Visit Hour Gap analysis for air_reserve"},{"metadata":{"trusted":true},"cell_type":"code","source":"hpgR2 = hpg_reserve_data.groupby(['visit_hour'], as_index=False).agg({'hpg_reserve_visitors': np.sum})\nhpgR3 = hpg_reserve_data.groupby(['res_vis_diff'], as_index=False).agg({'hpg_reserve_visitors': np.sum})\n\n#plot\nfig = plt.figure(figsize=(25, 6)) \ngs = gridspec.GridSpec(1, 2, width_ratios=[1.5, 2.5]) \nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nsns.barplot(x=\"visit_hour\",y=\"hpg_reserve_visitors\",data=hpgR2,ax=ax0)\nsns.barplot(x=\"res_vis_diff\",y=\"hpg_reserve_visitors\",data=hpgR3[(hpgR3['res_vis_diff'] <= 80)],ax=ax1)\nax0.set_xlabel('Visit Hour')\nax0.set_ylabel('Number of Reserve Visitor')\nax1.set_xlabel('Reservation-Visitor Hour Gap')\nax1.set_ylabel('Number of Reserve Visitor')\nfor ax in [ax0,ax1]:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations on Visit Hour:-**\n*   The hpg_reserve data also shows that most people make reservation for dinner (6pm to 8pm).\n*   There are very few reservations in the morning.\n\n**Observations on Reservation-Visit Hour Gap:-**\n*   Making reservation just a few hours before visiting the restaurant is comparable to the 24-hour pattern, whereas in air_reserve the number of reservation in the last moment was more.\n*   Very long time gaps between reservation and visit are not uncommon."},{"metadata":{},"cell_type":"markdown","source":"#### 6.3.3. Number of restaurants per area covered by HPG_RESERVE dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of restaurent in area: HPG store\nhpgS1=hpg_reserve_data['hpg_area_name'].value_counts().reset_index().sort_index()\nhpgS2=hpg_reserve_data['hpg_genre_name'].value_counts().reset_index().sort_index()\nfig,ax = plt.subplots(1,2)\nsns.barplot(y='index' ,x='hpg_area_name',data=hpgS1.iloc[:15],ax=ax[0])\nsns.barplot(y='index' ,x='hpg_genre_name',data=hpgS2.iloc[:15],ax=ax[1])\nfig.set_size_inches(20,7, forward=True)\nax[0].set_ylabel('Number of Restaurent')\nax[1].set_ylabel('Number of Restaurent')\nplt.tight_layout(pad=3.0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations on area:-**\n*    The hpg_restaurants in Japan are spreaded over 33 areas out of which the top 15 areas are shown above.\n*    Hokkaidō Asahikawa-shi 3 Jōdōri has the largest number of restaurants.\n\n**Observations on genre:-**\n*    The restaurants in Japan is subdivided in 14 food genres.\n*    Findings from the hpg_reserve dataset shows that Japanese style is the most popular genre. The second most popular genre for hpg restaurants is International Cuisine."},{"metadata":{},"cell_type":"markdown","source":"### 6.4. Number of Restaurants vs Visit Date in Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1, figsize=(15,6))\nstores= air_data.groupby(['visit_date'])['air_store_id'].size()\nstores.plot(kind='line',  color= 'chocolate', grid=True, ax=ax, legend=True)\nplt.ylabel(\"Number of Unique Resturant\")\nplt.xlabel(\"Visit Date\")\nplt.title(\"Number of Restaurants vs Visit Date in Train Data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n\n*        The above plot depicts the fact that there is almost 150% hike in the number of restaurants during mid of 2016.\n*        The reason behind the hike is that there is an addition of 500(approx) new restaurants to the AIR database in mid 2016.\n*        The ups and downs that we see is may be due to the weekday and weekends.\n*        The sharp decrese on 1st of Jan is due to the new-year's eve, as most of the restaurants stay closed on new-years day.\n*        In total we have records of almost 800 japanese restaurants."},{"metadata":{},"cell_type":"markdown","source":"### 6.5. Comparision between number of visitors and number of reservations"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(20,8))\nplt1 = air_data.groupby(['visit_date'])['visitors'].sum().to_frame()\nplt2 = air_reserve_data.groupby(['visit_date'])['air_reserve_visitors'].sum().to_frame()\nplt3 = hpg_reserve_data.groupby(['visit_date'])['hpg_reserve_visitors'].sum().to_frame()\nplt1.plot(color='salmon', kind='line', ax=ax)\nplt2.plot(color='cornflowerblue', kind='line', ax=ax)\nplt3.plot(color='y', kind='line', ax=ax)\nplt.legend()\nplt.ylabel(\"Sum of Visitors\")\nplt.xlabel(\"Visit Date\")\nplt.title(\"Visitor and Reservations\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n\n*        The abrupt hike in the middle of the 2016 is most probably due to the addition of new restaurants(as explained in previous plot).\n*        It can be easily observed that the number of non-registered visitors is far more than the number of registered visitors.\n*        There is a sharp decline at new years eve as most of the restaurants remain close on new year eve.\n*        The number of registration in AIR is more than that of HPG.\n*        The maximum numbers of visitors is observed in the month of December, since there are a number of festivals in December.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.6. Genre-wise comparison \n#### 6.6.1. Genre-wise comparison for visitors"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(15,6))\nplt1 = air_data.groupby('air_genre_name')['visitors'].sum().reset_index().nlargest(14, 'visitors')\nsns.barplot(x='visitors', y='air_genre_name', data=plt1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*   Izakaya has the highest number of visitors and hence is the most popular genre.\n*   Cafe/Sweets is the secondmost popular popular genre."},{"metadata":{},"cell_type":"markdown","source":"#### 6.6.2 Genre-wise comparison for hpg_reserve_visitors and air_reserve_visitors"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1, figsize=(15,6))\ngenre = air_data.groupby(['air_genre_name'])['air_reserve_visitors','hpg_reserve_visitors'].sum()\ngenre.sort_values(by=['air_reserve_visitors','hpg_reserve_visitors'],inplace=True)\ngenre.plot(kind='barh',color= ['salmon','cornflowerblue'], grid=True, ax=ax, legend=True)\nplt.ylabel('Genres')\nplt.xlabel('Reserve Visitors Count')\nplt.legend(loc='center right')\nplt.title(\"Reserve Visitors by Genre\", loc='center')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:-**\n*     Izakaya is the most popular genre. There are more reservations made using air service than that of hpg for Izakaya.\n*     Here Italian/French is the second most popular genre.\n*     Bar/Cocktail have comparable air and hpg reserve visitors.\n*     Aisan and International Cuisine are the least popular.\n*     Surprisingly the Japanese Food is the 4th popular genre in Japan.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.7. Average visitors on Holidays & Non-Holidays"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = air_data[['holiday_flg','visitors']].groupby(['holiday_flg'])['visitors'].mean()\ntemp.plot(kind='bar',color= ['green','blue'],figsize=(10,6))\nplt.ylabel('Average Visitors')\nplt.xlabel('Non-holiday=0 & Holiday=1')\nplt.title('Average visitors on Holidays & Non-Holidays')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:-**\n\n*        It is observed from the plot and it is obvious to have more visitors on holidays than working days.\n*        Even then the difference between the visitors on holidays and working days is not significantly large, which is due to weekend effect.\n*        It is also observed from the above plot that on most of the holidays, restaurants are open.\n*        While processing data, we must take into account the holidays that come on weekends, such holidays should only be considered as weekends not as holidays just to take the weekends effect into account.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.8. Area-wise Analysis of Train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import plugins\n\nlocation =air_data.groupby(['air_store_id','air_genre_name'])['latitude','longitude'].mean().reset_index()\nlocationlist = location[['latitude', 'longitude']]\nlocationlist = locationlist.values.tolist()\nmap2 = folium.Map(location=[39, 139], \n                        tiles = \"Stamen Toner\", width=1000, height=500,\n                        zoom_start = 5)\nmarker_cluster=plugins.MarkerCluster().add_to(map2)\nfor point in range(0, len(location)):\n    folium.Marker(locationlist[point], popup=location['air_genre_name'][point], \n    icon=folium.Icon(color='white', icon_color='red', \n                     #icon='fa fa-info-circle',\n                     icon='fa fa-circle-o-notch fa-spin',\n                     angle=0, \n                     prefix='fa')).add_to(marker_cluster)\nmap2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:-**\n*   This map shows the distribution of restaurants in Japan.\n*   The restaurants are located in 7 major areas. On zooming on each of them shows the distribution of restaurants in each of these location.\n*   Tokyo has the largest number of restaurants."},{"metadata":{},"cell_type":"markdown","source":"### 6.9. Distribution of visitors in each genre"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.FacetGrid(air_data, col=\"air_genre_name\", col_wrap=4, height=5, hue='air_genre_name',margin_titles=True,\n                  aspect=1.5, palette='husl', ylim=(0,150))\nax = ax.map(sns.lineplot, \"visit_date\", \"visitors\",  marker=\".\", linewidth = 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*  People visit the genres Izakaya, Cafe/Sweets, Italian/French and Dining bar consistently throughout the year and hence these genres have highest number of visitors.\n* Asian, International cuisine and Karaoke/Party are the new genres of restaurants that started in mid 2016.\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.10. Visitors Distribution of April, 2016\n\nWe have to predict the number of visitors for second and third week of April, 2017. So we are analyzing the number of visitors in the month of April,2016 as that may give rise to some monthly trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date = pd.to_datetime('2016-04-01')\nend_date = pd.to_datetime('2016-04-30')\nconfi_plot=air_data.loc[(air_data['visit_date'] > start_date) & (air_data['visit_date'] <= end_date), air_data.columns] \n\nconfidenceIntervals = [90, 50, 10]\n\nax = plt.subplots(1,1, figsize=(20,6))\n\ncolorPalette = sns.color_palette(\"ch:2.5,-.2,dark=.7\", n_colors=len(confidenceIntervals))[::-1]\n\nfor jj, ii in enumerate(confidenceIntervals):\n    ax = sns.lineplot(x=\"visit_date\", y=\"visitors\", data=confi_plot, ci=ii, label=str(ii), \n                      color=colorPalette[jj])\n\nax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:-**\n*   Confidence Interval refers to the probability that a population parameter will fall between a set of values for a certain proportion of time. Confidence intervals measure the degree of uncertainty or certainty in a sampling method. They can take any number of probability limits, with the most common being a 95% or 99% confidence level.\n*   Here we are showing the confidence in number of visitors on the month of april 2016 with the confidence level 10%,50% and 90%\n*   The points lying on the midddle line have a higher confidence level (90%). As the colour fades the confidence level decreases."},{"metadata":{},"cell_type":"markdown","source":"## 7. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 7.1. New Feature: Reserve_datetime_difference\n\n\n1. The air_store_id is mapped onto the corresponding hpg_store_id data from the store_id_relation.\n2. The reserve_datetime_difference column containing the difference between the visit_datetime and reserve_datetime is added.\n3. Derived columns: \n   - rdd_mean(mean of reserve_datetime_diff), \n   - rv_sum(total reserve visitor), \n   - rv_mean(mean of reserve visitors) are added by grouping the data on columns air_store_id and visit_datetime.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the air_store_id of corresponding hpg_stores from store_id_relation\nhpg_reserve = pd.merge(hpg_reserve,store_id_relation,on=['hpg_store_id'],how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hpg_reserve preprocessing\nhpg_reserve['visit_datetime'] = pd.to_datetime(hpg_reserve['visit_datetime'])\nhpg_reserve['visit_datetime'] = hpg_reserve['visit_datetime'].dt.date\nhpg_reserve['reserve_datetime'] = pd.to_datetime(hpg_reserve['reserve_datetime'])\nhpg_reserve['reserve_datetime'] = hpg_reserve['reserve_datetime'].dt.date\nhpg_reserve['reserve_datetime_diff'] = hpg_reserve.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n\ntmp1 = hpg_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_visitors':'rv_sum'})\ntmp2 = hpg_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rdd_mean', 'reserve_visitors':'rv_mean'})\nhpg_reserve= pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#air_reserve preprocessing\nair_reserve['visit_datetime'] = pd.to_datetime(air_reserve['visit_datetime'])\nair_reserve['visit_datetime'] = air_reserve['visit_datetime'].dt.date\nair_reserve['reserve_datetime'] = pd.to_datetime(air_reserve['reserve_datetime'])\nair_reserve['reserve_datetime'] = air_reserve['reserve_datetime'].dt.date\nair_reserve['reserve_datetime_diff'] = air_reserve.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n\ntmp1 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_visitors':'rv_sum'})\ntmp2 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rdd_mean', 'reserve_visitors':'rv_mean'})\nair_reserve= pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2. Extracting dow(day of week),  year, month from visit_date \n\n- In data visualization, we found out that number of visitors shows some trend depending on the day of week and month of year.\n- So, we are adding new columns dow, month and year to the train and test dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['visit_date'] = pd.to_datetime(train['visit_date'])\ntrain['dow'] = train['visit_date'].dt.dayofweek\ntrain['year'] = train['visit_date'].dt.year\ntrain['month'] = train['visit_date'].dt.month\ntrain['month_name'] = train['visit_date'].dt.month_name()\ntrain['visit_date'] = train['visit_date'].dt.date\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['visit_date'] = test['id'].map(lambda x: str(x).split('_')[2])\ntest['air_store_id'] = test['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ntest['visit_date'] = pd.to_datetime(test['visit_date'])\ntest['dow'] = test['visit_date'].dt.dayofweek\ntest['year'] = test['visit_date'].dt.year\ntest['month'] = test['visit_date'].dt.month\ntest['month_name'] = test['visit_date'].dt.month_name()\ntest['visit_date'] = test['visit_date'].dt.date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.2.2. Adding new feature non_working in date_info dataframe\n\n- As previously observed in data visualization, mean number of visitors in weekends is comparable to that in holidays. \n- A new feature, non_working is added, where along with holidays,  Saturday and Sunday are marked 1(non-working).\n- Significant difference in mean number of visitors between working and non-working days helps future prediction of number of visitors."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_info['non_working'] = np.where(date_info['day_of_week'].isin(['Saturday', 'Sunday']) | date_info['holiday_flg'] == 1, 1, 0)\ndate_info = date_info.drop('holiday_flg', axis = 1)\n\n#label encoding day_of_week which is a categorical data\nlbl = preprocessing.LabelEncoder()\ndate_info['day_of_week'] = lbl.fit_transform(date_info['day_of_week'])\n\ndate_info = date_info.rename(columns={'calendar_date':'visit_date'})\ndate_info['visit_date'] = pd.to_datetime(date_info['visit_date'])\ndate_info['visit_date'] = date_info['visit_date'].dt.date\n\ntrain_set = pd.merge(train, date_info, how='left', on=['visit_date'])\ntest_set = pd.merge(test, date_info, how='left', on=['visit_date']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2)\nfig.set_size_inches(15,4, forward=True)\n\nsns.boxplot(x = \"day_of_week\", y = \"visitors\", data=train_set,ax=ax[0])\nsns.boxplot(x = \"month_name\",y = \"visitors\", data=train_set,ax=ax[1])\nax[0].set_xlabel('Day of week')\nax[0].set_ylabel('Median Visitors')\nax[1].set_xlabel('Month of Year')\nax[1].set_ylabel('Median Visitors')\nfor ax in ax:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2)\nfig.set_size_inches(15,4, forward=True)\n\nmonth_day_log = train_set.copy()\nmonth_day_log['visitors'] =  np.log1p(month_day_log['visitors'])\n\nsns.boxplot(x = \"day_of_week\", y = \"visitors\", data=month_day_log,ax=ax[0])\nsns.boxplot(x = \"month_name\",y = \"visitors\", data=month_day_log,ax=ax[1])\nax[0].set_xlabel('Day of week')\nax[0].set_ylabel('Median Visitors')\nax[1].set_xlabel('Month of Year')\nax[1].set_ylabel('Median Visitors')\nfor ax in ax:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Effect of non_working feature\n- Significant difference in mean number of visitors between working and non-working days helps future prediction of number of visitors.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train_set[['non_working','visitors']].groupby(['non_working'])['visitors'].mean()\ntemp.plot(kind='bar',color= ['green','blue'],figsize=(10,6))\nplt.ylabel('Average Visitors')\nplt.xlabel('Working=0 & Non-working=1')\nplt.title('Average visitors on Working & Non-Working')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging hpg_reserve data with train and test set\ntrain_set = pd.merge(train_set, hpg_reserve, how='left', on=['air_store_id','visit_date']) \ntest_set = pd.merge(test_set, hpg_reserve, how='left', on=['air_store_id','visit_date'])\n\n#merging air_reserve data with train and test set\ntrain_set = pd.merge(train_set, air_reserve, how='left', on=['air_store_id','visit_date']) \ntest_set = pd.merge(test_set, air_reserve, how='left', on=['air_store_id','visit_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding air_store_id in train and test dataset as it is categorical value\nlbl = preprocessing.LabelEncoder()\ntrain_set['air_store_id2'] = lbl.fit_transform(train_set['air_store_id'])\ntest_set['air_store_id2'] = lbl.transform(test_set['air_store_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_set.fillna(0)\ntest_set = test_set.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging both the hpg_reserve and air_reserve dataset to train and test set introduces a pair of rv_sum, rv_mean and rdd_mean\n#so we add up total reserve visitors (rv_sum) \n#find the mean of pair of rv_mean\n#find the mean of pair of rdd_mean\n\ntrain_set['rv_sum'] = train_set['rv_sum_x'] + train_set['rv_sum_y']\ntrain_set['rv_mean'] = (train_set['rv_mean_x'] + train_set['rv_mean_y'])/2\ntrain_set['rdd_mean'] = (train_set['rdd_mean_x'] + train_set['rdd_mean_y'])/2\n\ntest_set['rv_sum'] = test_set['rv_sum_x'] + test_set['rv_sum_y']\ntest_set['rv_mean'] = (test_set['rv_mean_x'] + test_set['rv_mean_y'])/2\ntest_set['rdd_mean'] = (test_set['rdd_mean_x'] + test_set['rdd_mean_y'])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove the extra columns after updating the sum and mean in new columns\ntrain_set.drop(columns = {'rv_sum_x', 'rdd_mean_x', 'rv_mean_x', 'rv_sum_y', 'rdd_mean_y', 'rv_mean_y'}, inplace  = True)\ntest_set.drop(columns = {'rv_sum_x', 'rdd_mean_x', 'rv_mean_x', 'rv_sum_y', 'rdd_mean_y', 'rv_mean_y'}, inplace  = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2. Genre and Area wise restaurants grouping\n\n   We are creating a new feature that counts the number of restaurants of a particular genre in a particular area.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging store information with train and test set and label encoding the air_genre_name and air_area_name, which are categorical data\nlbl = preprocessing.LabelEncoder()\ntrain_set = pd.merge(train_set, air_store_info, on=['air_store_id'], how = 'left')\ntest_set = pd.merge(test_set, air_store_info, on=['air_store_id'], how = 'left')\n\ntrain_set['air_genre_name'] = lbl.fit_transform(train_set['air_genre_name'])\ntrain_set['air_area_name'] = lbl.fit_transform(train_set['air_area_name'])\n\ntest_set['air_genre_name'] = lbl.fit_transform(test_set['air_genre_name'])\ntest_set['air_area_name'] = lbl.fit_transform(test_set['air_area_name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.3. Working with Latitude and Longitude columns\n   **(Included in Final Improvements)**         "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set['var_max_lat'] = train_set['latitude'].max() - train_set['latitude']\ntrain_set['var_max_long'] = train_set['longitude'].max() - train_set['longitude']\ntest_set['var_max_lat'] = test_set['latitude'].max() - test_set['latitude']\ntest_set['var_max_long'] = test_set['longitude'].max() - test_set['longitude']\n\ntrain_set['lon_plus_lat'] = train_set['longitude'] + train_set['latitude'] \ntest_set['lon_plus_lat'] = test_set['longitude'] + test_set['latitude']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [c for c in train_set if c not in ['id', 'air_store_id', 'visit_date','visitors', 'month_name']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. MODEL BUILDING\n- We try out different models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(y, pred):\n    y = np.expm1(y)\n    pred = np.expm1(pred)\n    return np.sqrt(np.square(np.log(pred + 1) - np.log(y + 1)).mean())\n\nmodel1 = LinearRegression()\nmodel2 = DecisionTreeRegressor()\nmodel3 = RandomForestRegressor()     # New model used ( Included in the final improvements ) \nmodel4 = GradientBoostingRegressor()\nmodel5 = XGBRegressor()\nmodel6 = LGBMRegressor()    # New model used ( Included in the final improvements ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_decision={\n    \"max_depth\" : [1,2,3,4,5,6,None],\n    \"min_samples_split\" : [12,15,17,20],\n    \"max_leaf_nodes\" : [20,30,40],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(model2,param_grid = param_decision,n_jobs = -1)\ngrid.fit(train_set[col],train_set['visitors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_reg={\n    \"max_depth\" : [8,10,12],\n    #\"learning_rate\" : [0.1,0.2,0.3]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(model3,param_grid = param_reg,n_jobs = -1)\ngrid.fit(train_set[col],train_set['visitors'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(model4,param_grid = param_reg,n_jobs = -1)\ngrid.fit(train_set[col],train_set['visitors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(model5,param_grid = param_reg,n_jobs = -1)\ngrid.fit(train_set[col],train_set['visitors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(model6,param_grid = param_reg,n_jobs = -1)\ngrid.fit(train_set[col],train_set['visitors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LinearRegression()\nmodel2 = DecisionTreeRegressor(max_depth=3, max_leaf_nodes=20, min_samples_split=12)\nmodel3 = RandomForestRegressor(max_depth=8)\nmodel4 = GradientBoostingRegressor(max_depth=8)\nmodel5 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', \n             learning_rate=0.300000012, max_delta_step=0, max_depth=8,\n             min_child_weight=1,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nmodel6 = LGBMRegressor(max_depth=8,objective='regression', num_leaves=60, learning_rate=0.01, n_estimators=10000)\n\nmodel1.fit(train_set[col], np.log1p(train_set['visitors'].values))\nmodel2.fit(train_set[col], np.log1p(train_set['visitors'].values))\nmodel3.fit(train_set[col], np.log1p(train_set['visitors'].values))\nmodel4.fit(train_set[col], np.log1p(train_set['visitors'].values))\nmodel5.fit(train_set[col], np.log1p(train_set['visitors'].values))\nmodel6.fit(train_set[col], np.log1p(train_set['visitors'].values))\n\n\npreds1 = model1.predict(train_set[col])\npreds2 = model2.predict(train_set[col])\npreds3 = model3.predict(train_set[col])    # New model used ( Included in the final improvements )\npreds4 = model4.predict(train_set[col])\npreds5 = model5.predict(train_set[col])\npreds6 = model6.predict(train_set[col])    # New model used ( Included in the final improvements )\npreds56 =  (0.5*preds5) + (0.5*preds6)\n\nprint('RMSLE LinearRegressor: ', RMSLE(np.log1p(train_set['visitors'].values), preds1))\nprint('RMSLE DecisionTreeRegressor: ', RMSLE(np.log1p(train_set['visitors'].values), preds2))\nprint('RMSLE RadomForest: ', RMSLE(np.log1p(train_set['visitors'].values), preds3))\nprint('RMSLE GradientBoostingRegressor: ', RMSLE(np.log1p(train_set['visitors'].values), preds4))\nprint('RMSLE XGBRegressor: ', RMSLE(np.log1p(train_set['visitors'].values), preds5))\nprint('RMSLE LGBMRegressor: ', RMSLE(np.log1p(train_set['visitors'].values), preds6))\nprint('RMSLE Equally Weighted Ensemble: ', RMSLE(np.log1p(train_set['visitors'].values), preds56))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LINEAR REGRESSOR\n\n# preds1 = model1.predict(test_set[col])\n\n# test_set['visitors'] = preds1\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DECISION TREE REGRESSOR\n\n# preds2 = model2.predict(test_set[col])\n\n# test_set['visitors'] = preds2\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RANDOM FOREST\n\n# preds3 = model3.predict(test_set[col])\n\n# test_set['visitors'] = preds3\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADIENT BOOSTING REGRESSOR\n\n# preds2 = model2.predict(test_set[col])\n\n# test_set['visitors'] = preds2\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # XGBOOST REGRESSOR\n\n# preds5 = model5.predict(test_set[col])\n\n# test_set['visitors'] = preds5\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # LIGHT GBM\n\n# preds6 = model6.predict(test_set[col])\n\n# test_set['visitors'] = preds6\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensemble Model\n\n# preds56 = 0.5 * model3.predict(test_set[col]) + 0.5 * model4.predict(test_set[col])\n\n# test_set['visitors'] = preds5\n# test_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\n# sub1 = test_set[['id','visitors']].copy()\n# sub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the proper ensemble ratios to get minimum RMSLE ERROR \n**( Included in the final improvements )**"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_i = 0\nmin_rmsle = 1\nfor i in np.arange(0.0, 1.0, 0.001):\n    pred_i = i*preds5 + (1-i)*preds6\n    rmsle_i = RMSLE(np.log1p(train_set['visitors'].values), pred_i)\n    if(rmsle_i < min_rmsle):\n        min_rmsle = rmsle_i\n        min_i = i\n        \nprint('Min i = ', min_i)\nprint('Min rmsle = ', min_rmsle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Model : LGBM and XGBoost \n**(Included in the final improvements)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensemble Model : LGBM and XGBoost\n\npred_en = 0.317 * model5.predict(test_set[col]) + (1-0.317) * model6.predict(test_set[col])\n\ntest_set['visitors'] = pred_en\ntest_set['visitors'] = np.expm1(test_set['visitors']).clip(lower=0.)\nsub1 = test_set[['id','visitors']].copy()\nsub1.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"./submission.csv\")\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}